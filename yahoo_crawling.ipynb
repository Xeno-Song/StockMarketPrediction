{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Download FinBERT model from Hugging Face**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists(os.path.abspath('./finbert_model/pytorch_model.bin')) == False:\n",
    "    os.system('git lfs install')\n",
    "    os.system('git clone https://huggingface.co/ProsusAI/finbert finbert_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Add path to access FinBERT library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "finbert_path = os.path.abspath('./finbert')\n",
    "if (finbert_path in sys.path) == False:\n",
    "    sys.path.append(finbert_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Download NLTK tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import datetime\n",
    "from finbert.finbert import predict\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create data output directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './output'\n",
    "if os.path.exists(output_dir) == False:\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Access to Yahoo Finance page**  \n",
    "Open Yahoo finance page using chrome driver and get main page news contents.  \n",
    "News contents is wrapped by `container` class, so first get elements using this class info.  \n",
    "After that, find `a` tag to find link element, and get title and link.  \n",
    "In last, data will save as csv file.  \n",
    "\n",
    "#### **TODO**  \n",
    "Crawl news info from each category. Currently, this crawl main page news only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--blink-settings=imagesEnabled=false')\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "driver = webdriver.Chrome(chrome_options)\n",
    "driver.get('https://finance.yahoo.com/')\n",
    "driver.set_page_load_timeout(30)\n",
    "\n",
    "elements = driver.find_elements(By.CLASS_NAME, 'container')\n",
    "\n",
    "news_titles = []\n",
    "for element in elements:\n",
    "    # content_card = element.find_elements(By.CLASS_NAME, 'content')\n",
    "    title_part = element.find_elements(By.TAG_NAME, 'a')\n",
    "    if len(title_part) == 0:\n",
    "        continue\n",
    "\n",
    "    title = title_part[0].get_attribute('title')\n",
    "    link = title_part[0].get_attribute('href')\n",
    "    news_titles.append((title, link))\n",
    "\n",
    "output_path = os.path.abspath('./output/' + datetime.datetime.now().strftime('%Y%m%d_%H%M%S') + '.csv')\n",
    "with open(output_path, '+w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join([index[0] for index in news_titles]))\n",
    "    f.close()\n",
    "\n",
    "print(\"Main page news loaded.\")\n",
    "print(f\"{len(elements)} contents found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Load each articles contents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_index = []\n",
    "\n",
    "for target_article in news_titles:\n",
    "    driver.get(target_article[1])\n",
    "    article_content_area = driver.find_elements(By.CLASS_NAME, 'caas-body')\n",
    "    if len(article_content_area) == 0:\n",
    "        continue\n",
    "\n",
    "    article_content = []\n",
    "    contents_components = article_content_area[0].find_elements(By.TAG_NAME, 'p')\n",
    "\n",
    "    for component in contents_components:\n",
    "        content = component.text\n",
    "        if len(content) != 0:\n",
    "            article_content.append(content)\n",
    "    \n",
    "    articles_index.append((target_article[0], article_content))\n",
    "    print(\"Article parsed : \" + target_article[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Save loaded article contents as file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "article_base_path = os.path.abspath('./output/' + datetime.datetime.now().strftime('%Y%m%d_%H%M%S') + '/') + '/'\n",
    "os.mkdir(article_base_path)\n",
    "\n",
    "for index in range(0, len(articles_index)):\n",
    "    article = articles_index[index]\n",
    "    article_path = article_base_path + str(index) + '.txt'\n",
    "\n",
    "    with open(article_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(article[0] + '\\n\\n')\n",
    "        f.write('\\n'.join([index for index in article[1]]))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Predict positive/negative using FinBERT model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.abspath('./finbert_model')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=3, cache_dir=None)\n",
    "\n",
    "output_path = os.path.abspath('./output/' + datetime.datetime.now().strftime('%Y%m%d_%H%M%S') + '_output/') + '/'\n",
    "os.mkdir(output_path)\n",
    "for index in range(0, len(articles_index)):\n",
    "    article = articles_index[index]\n",
    "    output_filepath = output_path + str(index) + '.csv'\n",
    "\n",
    "    print(article[1])\n",
    "    predict('\\n'.join(article[1]), model, write_to_csv=True, path=output_filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
